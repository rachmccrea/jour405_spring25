---
title: "HW9_TestScores"
name: RACHEL MCCREA
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Did a New Reading Program Lead to Better Scores?

The superintendent recently claimed that a new reading program has improved third-grade reading scores across the school district.

Before the program, third-grade students in the district averaged 72.6 points on standardized reading tests with a standard deviation of 4.8 points.

After implementing the program for one semester, you collected scores from 12 randomly selected classrooms:
74, 76, 73, 75, 78, 77, 74, 79, 75, 76, 77, 75

As a journalist, you need to determine: **Is there statistical evidence that reading scores have actually improved?**

## Task 1: Organize your data and initial assessment

Before you can run this codeblock, you will need to fill in a value where it says REPLACE_ME. That value can be found in the introduction.

```{r}
# Known information about reading scores before the new program
prior_mean <- 72.6
prior_sd <- 4.8

# Reading scores after implementing the new program (12 classrooms)
new_scores <- c(74, 76, 73, 75, 78, 77, 74, 79, 75, 76, 77, 75) # Replace with the actual scores

new_mean <- mean(c(74, 76, 73, 75, 78, 77, 74, 79, 75, 76, 77, 75))  # average score = 75.75
new_sd <- sd(c(74, 76, 73, 75, 78, 77, 74, 79, 75, 76, 77, 75))    # standard deviation = 1.764549...
#had to look this up -- google reminded me I had to make this a list instead of just plugging the values in. 

# Create a journalist-friendly dataset
score_data <- tibble(
  classroom = paste("Classroom", 1:12),
  reading_score = new_scores
)

# View the data
score_data
```

### Reflection Question 1:
Based on just looking at the score_data dataframe, have test scores improved? How can you tell?

I think these test scores have improved, because each classroom has a score above the prior mean of 72.6

## Task 2: Calculate key statistics

Like Task 1, you will need to replace values where it says REPLACE_ME before running any code.


```{r}
# Calculate statistics based on the new reading scores
new_stats <- score_data |> 
  summarise(
    mean = mean(new_scores),
    sd = sd(new_scores),
    n = n()
  )

new_stats
```

### Reflection Question 2:
Looking at the mean and standard deviation of the new scores compared to the previous statistics, what initial observations can you make? What questions might these statistics raise for your reporting?

The new mean is higher than the prior mean by a few points! The new SD is smaller than the prior SD, which means there's less variation around the mean. I think that means that all the classroom scores are closer to each other. What's the cause of this? Does this mean classrooms with lower scores caught up to classrooms with higher scores, or that scores dropped in higher classrooms? Did this change happen after implementation of the reading program? how long after? What's the p-value? 

## Task 3: Create a column chart

As before, replace any values marked REPLACE_ME based the instructions.


```{r}
# STUDENT TASK: Choose an appropriate fill color for the bars
my_fill_color <- "darkgreen" # Replace with a color name like "royalblue", "darkgreen", etc.

# Create a visualization comparing new scores to the previous average
score_data |> 
ggplot(aes(x = classroom, y = reading_score)) +
  geom_col(fill = my_fill_color, alpha = 0.8) +
  geom_hline(yintercept = prior_mean, color = "darkred", size = 1, linetype = "dashed") +
  annotate("text", x = 2, y = prior_mean - 1, 
           label = "Previous Average (72.6)", hjust = 0, fontface = "bold", color = "darkred") +
  labs(
    title = "Reading Scores After New Program Implementation",
    subtitle = "Horizontal line shows previous district average of 72.6 points",
    x = NULL,
    y = "Reading Test Score",
    caption = "Source: District Assessment Data"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

### Reflection Question 3:
Examine the chart you created, and suggest a better title based on the results of the data, not a description.

Classroom reading scores improve slightly after new program implementation

## Task 4: Perform a hypothesis test

This is where we formally test the superintendent's claim that reading scores have improved. Fill in the REPLACE_ME values as needed, beginning with your hypotheses.

**Hypotheses:**
Null: The new reading program did not improve reading scores
Alternative: The new reading program did improve reading scores 

```{r}
# Set the significance level for your test
alpha_level <- 0.05 # Replace with the appropriate value

# Perform a one-sample t-test
# Since we want to know if scores improved (increased), we use a one-sided test (alternative = "greater")
t_test_result <- t.test(
  score_data$reading_score,
  mu = prior_mean,
  alternative = "greater"
)

# Display the results
t_test_result
```

### Reflection Question 4:
What does the p-value tell you, and what doesn't it tell you? How would you explain these results to a non-technical audience while maintaining accuracy?

P-value is 0.00003435, which is less than the alpha. That means it's statistically significant -- the reading program didn't not improve scores. 

P-value doesn't tell us that the reading program DID improve scores -- it's up to us to figure out whether there's a third variable influencing this. 

I'd explain it to a non-technical audience like this: 

A data analysis performed by this news organization found that the reading program could have caused these improvements. However, it's unclear whether there was another reason reading scores could have risen 

^^I'm not totally sure that's how I'm supposed to explain it.^^ 


## Task 5: Interpreting the results for your news story

Let's gather all of the important stats we'll need in one place, so we can look at the prior average, the new scores and the results of the t.test, including the confidence interval. Replace any values where it says REPLACE_ME.


```{r}
# Get the p-value
p_value <- t_test_result$p.value

# Calculate the 95% confidence interval
ci <- t.test(score_data$reading_score)$conf.int

# Create a tibble to display the key statistics for your story
story_stats <- tibble(
  `Previous average` = prior_mean,
  `New average` = mean(new_mean),
  `Improvement` = mean(new_scores) - prior_mean,
  `Percent change` = round(((mean(new_scores) - prior_mean) / prior_mean) * 100, 1),
  `p-value` = p_value,
  `Lower bound` = ci[1],
  `Upper bound` = ci[2],
  `Confidence level` = "95%"
)

# Display the key statistics
story_stats
```

## Conclusion

### Reflection Question 5:
Based on these statistics, what would be your headline and lead paragraph for this story? Is there evidence to support the superintendent's claim?

Headline: Student  scores show improvement after start of new reading program

Lede: Student reading scores improved after DISTRICT NAME implemented a new program. The average reading score rose 4.3% after the program began, according to district data. 

While there's no guarantee that the program directly caused scores to rise, a data analysis by NEWS ORGANIZATION found a significant chance that the two are related. 

The average student score before the program started was a 72.6. After the program was implemented, students were scoring at 75.75 on average. 


### Reflection Question 6:
What metrics or outcomes beyond test scores might be important to track for assessing reading performance?

 Reading comprehension is  super important for this -- it doesn't matter how high test scores are if comprehension isn't good. Also class attendance rates, the amount of time studnets spend reading both in and out of class, the length and complexity of the books they read, their reading levels, and their own confidence in their reading skills. If we can measure enjoyment of reading, that could also be a fun stat to look at. 



